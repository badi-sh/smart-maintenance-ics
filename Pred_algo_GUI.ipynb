{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zw-xtLMKMLws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e583c1b-4421-489b-ffa7-6c7ec1a03d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1367022633.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'SAFE' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  df1.fillna(\"SAFE\",inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 15) (10000, 15) (20000, 15)\n"
          ]
        }
      ],
      "source": [
        "# CSV merger\n",
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('/content/fail.csv')\n",
        "df1=pd.read_csv('/content/non_fail.csv')\n",
        "\n",
        "df.fillna(\"SAFE\",inplace=True)\n",
        "df1.fillna(\"SAFE\",inplace=True)\n",
        "\n",
        "l1,l2=df.columns.tolist(),df1.columns.tolist()\n",
        "for i in range(len(l1)):\n",
        "  if l1[i]!=l2[i]:\n",
        "    print(l1[i],l2[i])\n",
        "\n",
        "df_merged=pd.concat([df,df1],axis=0)\n",
        "df_merged = df_merged.sample(frac=1).reset_index(drop=True)\n",
        "print(df.shape,df1.shape,df_merged.shape)\n",
        "df_merged.to_csv(\"merged.csv\", encoding='utf-8', index=False, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kcZxsbMCrzmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "qa-2WP77FO4P",
        "outputId": "714d2afd-916c-484b-e0d5-bef2ae34b4cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c13b18731f4f47dd50.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c13b18731f4f47dd50.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c13b18731f4f47dd50.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Set up directories for saving plots and models\n",
        "output_dir = Path(\"plots\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "model_dir = Path(\"models\")\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Training function\n",
        "def train_model(training_csv):\n",
        "    if training_csv is None:\n",
        "        return \"Please upload a training CSV file.\", None, None, None, None, None, None\n",
        "\n",
        "    # Load training data\n",
        "    data = pd.read_csv(training_csv)\n",
        "    data = data.drop([\"failure\", \"failure_type\"], axis=1)\n",
        "\n",
        "    # Define features and target\n",
        "    X = data.drop(\"failure_transition\", axis=1)\n",
        "    y = data[\"failure_transition\"]\n",
        "\n",
        "    # Handle missing or invalid data\n",
        "    X = X.fillna(X.mean())\n",
        "    y = y.fillna(y.mode()[0])\n",
        "\n",
        "    # Ensure all features are numerical\n",
        "    X = X.apply(pd.to_numeric, errors='coerce')\n",
        "    X = X.fillna(X.mean())\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, model_dir / 'scaler.pkl')\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(model, model_dir / 'random_forest_model.pkl')\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Classification report\n",
        "    clf_report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # 1. Confusion Matrix Heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    confusion_matrix_path = output_dir / 'confusion_matrix.png'\n",
        "    plt.savefig(confusion_matrix_path)\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Feature Importance Plot\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "    plt.title('Feature Importance')\n",
        "    feature_importance_path = output_dir / 'feature_importance.png'\n",
        "    plt.savefig(feature_importance_path)\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Learning Curves\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        model, X_scaled, y, cv=5, scoring='accuracy', n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), random_state=42\n",
        "    )\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, label='Training Accuracy')\n",
        "    plt.plot(train_sizes, val_mean, label='Validation Accuracy')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
        "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
        "    plt.title('Learning Curves')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True)\n",
        "    learning_curves_path = output_dir / 'learning_curves.png'\n",
        "    plt.savefig(learning_curves_path)\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Class Distribution Bar Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x=y, order=y.value_counts().index)\n",
        "    plt.title('Class Distribution in failure_transition')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    class_distribution_path = output_dir / 'class_distribution.png'\n",
        "    plt.savefig(class_distribution_path)\n",
        "    plt.close()\n",
        "\n",
        "    return (clf_report, str(confusion_matrix_path), str(feature_importance_path),\n",
        "            str(learning_curves_path), str(class_distribution_path), model, scaler)\n",
        "\n",
        "# Inference engine\n",
        "def inference_engine(model, scaler, input_data=None, csv_file=None):\n",
        "    feature_columns = ['time', 'rpm', 'pressure', 'flow_rate', 'temperature',\n",
        "                       'vibration', 'current', 'noise', 'ambient_temp',\n",
        "                       'humidity', 'fluid_viscosity', 'external_vibration']\n",
        "\n",
        "    if csv_file is not None:\n",
        "        new_data = pd.read_csv(csv_file)\n",
        "        new_data = new_data[feature_columns]\n",
        "    elif input_data is not None:\n",
        "        new_data = pd.DataFrame([input_data], columns=feature_columns)\n",
        "    else:\n",
        "        raise ValueError(\"Either input_data or csv_file must be provided\")\n",
        "\n",
        "    new_data = new_data.fillna(new_data.mean())\n",
        "    new_data = new_data.apply(pd.to_numeric, errors='coerce')\n",
        "    new_data = new_data.fillna(new_data.mean())\n",
        "    new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "    predictions = model.predict(new_data_scaled)\n",
        "    probabilities = model.predict_proba(new_data_scaled)\n",
        "\n",
        "    result_df = new_data.copy()\n",
        "    result_df['predicted_failure_transition'] = predictions\n",
        "    for i, class_name in enumerate(model.classes_):\n",
        "        result_df[f'prob_{class_name}'] = probabilities[:, i]\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Predictive maintenance module\n",
        "def predictive_maintenance(result_df):\n",
        "    label_counts = result_df['predicted_failure_transition'].value_counts()\n",
        "    label_proportions = label_counts / len(result_df)\n",
        "\n",
        "    # Plot label distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=label_counts.values, y=label_counts.index)\n",
        "    plt.title('Predicted Failure Transition Distribution')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('Failure Transition')\n",
        "    label_distribution_path = output_dir / 'inference_label_distribution.png'\n",
        "    plt.savefig(label_distribution_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Estimate data duration\n",
        "    time_span_hours = result_df['time'].max() - result_df['time'].min()\n",
        "    duration_text = f\"Data Duration: {time_span_hours:.2f} hours\"\n",
        "\n",
        "    # Maintenance timeframe logic\n",
        "    critical_labels = ['IMPELLER_DAMAGE', 'BEARING', 'CAVITATION']\n",
        "    moderate_labels = ['OVERLOAD', 'SEAL_LEAK']\n",
        "\n",
        "    critical_proportion = label_proportions[critical_labels].sum() if any(l in label_proportions for l in critical_labels) else 0\n",
        "    moderate_proportion = label_proportions[moderate_labels].sum() if any(l in label_proportions for l in moderate_labels) else 0\n",
        "    safe_proportion = label_proportions.get('SAFE', 0)\n",
        "\n",
        "    routine_maintenance = 1500\n",
        "    major_overhaul = 10000\n",
        "\n",
        "    if critical_proportion > 0.1:\n",
        "        timeframe = \"Immediate (within 1–7 days)\"\n",
        "        urgency = \"Urgent\"\n",
        "        urgency_reason = f\"High proportion of critical failures ({critical_proportion:.2%})\"\n",
        "    elif moderate_proportion > 0.2:\n",
        "        timeframe = \"Within 100–500 hours (~4–20 days, assuming 24/7 operation)\"\n",
        "        urgency = \"High\"\n",
        "        urgency_reason = f\"Significant proportion of moderate failures ({moderate_proportion:.2%})\"\n",
        "    elif safe_proportion > 0.8:\n",
        "        timeframe = f\"Next routine maintenance (~{routine_maintenance} hours)\"\n",
        "        urgency = \"Low\"\n",
        "        urgency_reason = f\"Predominantly SAFE operation ({safe_proportion:.2%})\"\n",
        "    else:\n",
        "        timeframe = \"Within 500–1000 hours (~20–40 days, assuming 24/7 operation)\"\n",
        "        urgency = \"Moderate\"\n",
        "        urgency_reason = \"Mixed failure types detected\"\n",
        "\n",
        "    # Maintenance recommendations, prioritized by severity\n",
        "    recommendations = []\n",
        "    critical_recs = []\n",
        "    moderate_recs = []\n",
        "    safe_rec = None\n",
        "\n",
        "    for label in label_counts.index:\n",
        "        if label == 'IMPELLER_DAMAGE':\n",
        "            critical_recs.append(\"Inspect/replace impeller; check for erosion, imbalance, or material wear.\")\n",
        "        elif label == 'BEARING':\n",
        "            critical_recs.append(\"Inspect/replace bearings; verify lubrication and alignment.\")\n",
        "        elif label == 'CAVITATION':\n",
        "            critical_recs.append(\"Adjust flow conditions; inspect impeller and casing for pitting.\")\n",
        "        elif label == 'OVERLOAD':\n",
        "            moderate_recs.append(\"Check motor, electrical systems, and load conditions.\")\n",
        "        elif label == 'SEAL_LEAK':\n",
        "            moderate_recs.append(\"Replace seals; inspect for wear or misalignment.\")\n",
        "        elif label == 'SAFE':\n",
        "            safe_rec = f\"SAFE operation detected ({label_proportions['SAFE']:.2%} of samples); continue regular monitoring.\"\n",
        "\n",
        "    # Combine recommendations based on urgency\n",
        "    if urgency in [\"Urgent\", \"High\"]:\n",
        "        recommendations = critical_recs + moderate_recs\n",
        "        if safe_rec and safe_proportion > 0:\n",
        "            recommendations.append(f\"Note: {safe_rec}\")\n",
        "    elif urgency == \"Moderate\":\n",
        "        recommendations = critical_recs + moderate_recs\n",
        "        if safe_rec and safe_proportion > 0:\n",
        "            recommendations.append(f\"Note: {safe_rec}\")\n",
        "    else:  # Low urgency\n",
        "        recommendations = critical_recs + moderate_recs\n",
        "        if safe_rec:\n",
        "            recommendations.append(safe_rec)\n",
        "        else:\n",
        "            recommendations.append(\"Continue regular monitoring.\")\n",
        "\n",
        "    # Format maintenance analysis\n",
        "    maintenance_text = f\"\"\"\n",
        "Predictive Maintenance Analysis:\n",
        "Label Distribution:\n",
        "{label_counts.to_string()}\n",
        "Label Proportions:\n",
        "{label_proportions.to_string()}\n",
        "Urgency: {urgency}\n",
        "Reason: {urgency_reason}\n",
        "Recommended Maintenance Timeframe: {timeframe}\n",
        "Maintenance Recommendations:\n",
        "\"\"\" + \"\\n\".join(f\"- {rec}\" for rec in recommendations if rec)\n",
        "\n",
        "    # Timeline plot\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    plt.axvline(0, color='green', label='Current Time')\n",
        "    if 'Immediate' in timeframe:\n",
        "        plt.axvspan(0, 168, alpha=0.3, color='red', label='Maintenance Window')\n",
        "    elif '100–500' in timeframe:\n",
        "        plt.axvspan(100, 500, alpha=0.3, color='orange', label='Maintenance Window')\n",
        "    elif '500–1000' in timeframe:\n",
        "        plt.axvspan(500, 1000, alpha=0.3, color='yellow', label='Maintenance Window')\n",
        "    else:\n",
        "        plt.axvspan(routine_maintenance - 100, routine_maintenance + 100, alpha=0.3, color='green', label='Maintenance Window')\n",
        "    plt.title('Estimated Maintenance Timeline')\n",
        "    plt.xlabel('Operating Hours from Now')\n",
        "    plt.yticks([])\n",
        "    plt.legend()\n",
        "    timeline_path = output_dir / 'maintenance_timeline.png'\n",
        "    plt.savefig(timeline_path)\n",
        "    plt.close()\n",
        "\n",
        "    return maintenance_text, str(label_distribution_path), str(timeline_path), duration_text, result_df\n",
        "\n",
        "# Gradio interface function\n",
        "def run_predictive_maintenance(training_csv, inference_csv):\n",
        "    # Train the model\n",
        "    (clf_report, confusion_matrix_path, feature_importance_path,\n",
        "     learning_curves_path, class_distribution_path, model, scaler) = train_model(training_csv)\n",
        "\n",
        "    if model is None:\n",
        "        return (clf_report, None, None, None, None, None, None, None, None, None)\n",
        "\n",
        "    # Run inference and predictive maintenance\n",
        "    result_df = inference_engine(model, scaler, csv_file=inference_csv)\n",
        "    maintenance_text, label_distribution_path, timeline_path, duration_text, result_df = predictive_maintenance(result_df)\n",
        "\n",
        "    # Save inference results\n",
        "    inference_results_path = output_dir / 'inference_results.csv'\n",
        "    result_df.to_csv(inference_results_path, index=False)\n",
        "\n",
        "    return (clf_report, confusion_matrix_path, feature_importance_path,\n",
        "            learning_curves_path, class_distribution_path, maintenance_text,\n",
        "            label_distribution_path, timeline_path, duration_text, result_df)\n",
        "\n",
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Centrifuge Predictive Maintenance System\")\n",
        "    gr.Markdown(\"Upload the training dataset and an inference CSV to analyze centrifuge health and predict maintenance needs.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            training_csv = gr.File(label=\"Upload Training CSV (merged_dataset.csv)\")\n",
        "            inference_csv = gr.File(label=\"Upload Inference CSV\")\n",
        "            submit_button = gr.Button(\"Run Analysis\")\n",
        "\n",
        "        with gr.Column():\n",
        "            clf_report = gr.Textbox(label=\"Classification Report\")\n",
        "            duration_text = gr.Textbox(label=\"Data Duration\")\n",
        "            maintenance_text = gr.Textbox(label=\"Predictive Maintenance Analysis\")\n",
        "\n",
        "    with gr.Row():\n",
        "        confusion_matrix_img = gr.Image(label=\"Confusion Matrix\")\n",
        "        feature_importance = gr.Image(label=\"Feature Importance\")\n",
        "\n",
        "    with gr.Row():\n",
        "        learning_curves = gr.Image(label=\"Learning Curves\")\n",
        "        class_distribution = gr.Image(label=\"Class Distribution\")\n",
        "\n",
        "    with gr.Row():\n",
        "        label_distribution = gr.Image(label=\"Inference Label Distribution\")\n",
        "        timeline = gr.Image(label=\"Maintenance Timeline\")\n",
        "\n",
        "    inference_results = gr.Dataframe(label=\"Inference Results\")\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=run_predictive_maintenance,\n",
        "        inputs=[training_csv, inference_csv],\n",
        "        outputs=[clf_report, confusion_matrix_img, feature_importance,\n",
        "                 learning_curves, class_distribution, maintenance_text,\n",
        "                 label_distribution, timeline, duration_text, inference_results]\n",
        "    )\n",
        "\n",
        "# Launch Gradio interface\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM4n5yvmg_oJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}